defaults:
  - /base@_here_
  - /data@_here_
  - _self_

name: mlp
experiment_name: mlp_classifier
seed: 420
N: 100  # data dimension

# Model parameters
model:
  hidden_dims: [1000]  # Hidden layer dimensions
  dropout_rate: 0.0       # Dropout probability

# Dataloader parameters
dataloader:
  batch_size: 32

# Optimizer parameters
optimizer:
  name: "adamw"  # Options: adam, adamw, sgd
  learning_rate: 0.0005
  weight_decay: 0

# Learning rate scheduler
scheduler:
  enabled: false
  name: "plateau"  # Options: step, cosine, plateau
  params:
    factor: 0.5
    patience: 5
    
# Training parameters
trainer:
  max_epochs: 50
  accelerator: "auto"  # Options: cpu, gpu, auto
  devices: 1
  precision: 32  # Options: 32, 16, "bf16"

# Checkpointing parameters
checkpoint_callback:
  enabled: false
  monitor: "val_loss"
  mode: "min"
  save_top_k: 1
  save_last: true
  filename: "epoch={epoch:02d}-val_loss={val_loss:.4f}"
  dirpath: "checkpoints"

# Early stopping parameters
early_stopping:
  enabled: true
  monitor: "train_loss"
  min_delta: 0.0001
  patience: 20
  mode: "min"

# Logging parameters
logging:
  dir: "lightning-logs"
  log_every_n_steps: 10

# Hamming balls data
data:
  synthetic:
    P: 30   # Patterns per class (train + val + test)
    C: 10   # Number of classes
    p: 0.3  # Probability of flipping bits from prototypes
    save_dir: "data/balanced_dataset"
    load_if_available: true
    dump: true
    shuffle: true
    val_split: 0.335  # Validation split ratio
    test_split: 0.33  # Test split ratio