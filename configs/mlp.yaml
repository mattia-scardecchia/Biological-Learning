# Hydra configuration file for MLP Classifier

# Experiment settings
experiment_name: mlp_classifier
seed: 42

# Dataset parameters
dataset:
  N: 100  # Input dimensionality
  P: 30   # Patterns per class (train + val + test)
  C: 10   # Number of classes
  p: 0.3  # Probability of flipping bits from prototypes
  save_dir: "data/balanced_dataset"
  load_if_available: true
  dump: true
  shuffle: true

# Model parameters
model:
  hidden_dims: [128, 64]  # Hidden layer dimensions
  dropout_rate: 0.2       # Dropout probability

# Dataloader parameters
dataloader:
  batch_size: 32
  val_split: 0.33  # Validation split ratio
  test_split: 0.33  # Test split ratio

# Optimizer parameters
optimizer:
  name: "adam"  # Options: adam, adamw, sgd
  learning_rate: 0.001
  weight_decay: 1e-5

# Learning rate scheduler
scheduler:
  enabled: true
  name: "plateau"  # Options: step, cosine, plateau
  params:
    factor: 0.5
    patience: 5
    
# Training parameters
trainer:
  max_epochs: 100
  accelerator: "auto"  # Options: cpu, gpu, auto
  devices: 1
  precision: 32  # Options: 32, 16, "bf16"

# Checkpointing parameters
checkpoint_callback:
  enabled: true
  monitor: "val_loss"
  mode: "min"
  save_top_k: 1
  save_last: true
  filename: "epoch={epoch:02d}-val_loss={val_loss:.4f}"
  dirpath: "checkpoints"

# Early stopping parameters
early_stopping:
  enabled: true
  monitor: "val_loss"
  min_delta: 0.0001
  patience: 10
  mode: "min"

# Logging parameters
logging:
  dir: "logs"
  log_every_n_steps: 10