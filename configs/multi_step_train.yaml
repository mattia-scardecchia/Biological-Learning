defaults:
  - /base@_here_
  - _self_
  - /data@_here_

# NOTE: full lists take precedence (e.g. for lr, weight_decay, threshold, lambdas, etc.)

# Experiment
name: 'prova'
seed: 17
device: 'mps'

# Network and Relaxation
N: 100
H: 300
num_layers: 3
J_D: 0.5
max_steps_train: 7                             # full sweeps over the network (relaxation)
max_steps_eval: ${max_steps_train}
init_mode: "zeros"                             # "input", "zeros"
init_noise: 0.0

# lambda_left: [6.0, 4.0, 4.0, 4.0, 4.0, 1.0]     # first is lambda_x, last affects the readout layer
# lambda_right: [4.0, 4.0, 4.0, 4.0, 1.0, 6.0]  # last is lambda_y, last but one affects the readout layer
lambda_x: 0.0
lambda_y: 1000.0
lambda_l: 1.0
lambda_r: ${lambda_l}
lambda_internal: [0.0, 0.0, 0.0]
lambda_fc: 1.0
lambda_wback_skip: [0.0, 0.0]
lambda_wback: 2.0  # (/100 if buggy)
lambda_input_skip: [2.0, 0.0, 0.0]
lambda_input_output_skip: 0.0  # multiplied by L
lambda_wforth_skip: [0.0, 0.0]
lambda_wforth: 1.0

fc_left: true
fc_right: false
fc_input: false
zero_fc_init: true
symmetrize_fc: false
symmetric_W: false
symmetric_J_init: false

double_dynamics: false
double_update: false
use_local_ce: false
p_update: 0.8
beta_ce: 2.5
lambda_cylinder: null

# Perceptron Rule
batch_size: 16
# lr: [0.03, 0.03, 0.03, 0.0, 0.1]                        # scaled by typical weight size. last two are for W_back and W_forth
# threshold: [1.0, 1.0, 1.0, 3.0]                       # perceptron rule. last is for readout layer
# weight_decay: [0.005, 0.005, 0.0, 0.005]              # scaled by learning rate. last two are for W_back and W_forth
lr_input_output_skip: 0.1
lr_input_skip: 0.03
lr_J: 0.03
lr_W: 0.1
lr_wforth_skip: 0.1

threshold_hidden: 0.5
threshold_readout: 3.0
weight_decay_J: 0.005
weight_decay_W: 0.005
weight_decay_wforth_skip: 0.005
weight_decay_input_skip: 0.0
weight_decay_input_output_skip: 0.005

# Phases
num_epochs_warmup: 10
num_epochs_couplings: 40
num_epochs_full: 0
num_epochs_tuning: 20

begin_curriculum: 1.0  # in [0,1). if >= 1, no curriculum. affects second phase only
p_curriculum: 0.5    # affects second phase only
begin_curriculum_tuning: 1.0  # in [0,1). if >= 1, no curriculum. affects third phase only
p_curriculum_tuning: 0.5    # affects third phase only

# Evaluation
eval_interval: 1  # epochs
skip_representations: false
skip_couplings: false
skip_fields: false
skip_overlaps: true

bias_std: 0.0

save_model_and_data: false
inference_ignore_right: 1  # for inference mode