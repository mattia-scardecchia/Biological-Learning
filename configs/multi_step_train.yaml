defaults:
  - /base@_here_
  - _self_
  - /data@_here_

# NOTE: full lists take precedence (e.g. for lr, weight_decay, threshold, lambdas, etc.)

# Experiment
name: 'prova'
seed: 23
device: 'mps'

# Network and Relaxation
N: 100
H: 500
num_layers: 1
J_D: 0.5
max_steps: 5                                   # full sweeps over the network (relaxation)
init_mode: "zeros"                             # "input", "zeros"
fc_left: true
fc_right: false
# lambda_left: [6.0, 4.0, 4.0, 4.0, 4.0, 1.0]     # first is lambda_x, last affects the readout layer
# lambda_right: [4.0, 4.0, 4.0, 4.0, 1.0, 6.0]  # last is lambda_y, last but one affects the readout layer
lambda_x: 100.0
lambda_y: 100.0
lambda_l: 4.0
lambda_r: 2.0
lambda_wback: 0.1
lambda_internal: 1.0
lambda_fc: 0.001
symmetric_W: false

# Perceptron Rule
batch_size: 16
lr: [0.05, 0.0, 0.5]                        # scaled by typical weight size. last two are for W_back and W_forth
threshold: [1.0, 7.5]                       # perceptron rule. last is for readout layer
weight_decay: [0.02, 0.0, 0.005]              # scaled by learning rate. last two are for W_back and W_forth
# lr_J: 0.1
# lr_W: 0.1
# threshold_hidden: -10.0
# threshold_readout: 7.5
# weight_decay_J: 0.02
# weight_decay_W: 0.02

# Phases
num_epochs_warmup: 3
num_epochs_full: 0
num_epochs_couplings: 10
num_epochs_tuning: 10

# Evaluation
eval_interval: 1  # epochs
skip_representations: false
skip_couplings: false
skip_fields: false
