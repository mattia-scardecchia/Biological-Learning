TODO:

- our weight decay strat hinders learning. maybe smooth l1? boh
- Test in notebook with a single wide hidden layer!
    - readout weights:
        - initialize symmetric but make independent copies, both learn
        - initialize symmetric but keep wback fixed (lr 0)
        - make symmetric and learn only with W_forth step
    - how big should all multipliers/fields be?
    - init zeros/input?
    - relative learning speed of readut weights and couplings:
        - different learning rates?
        - initial warmup with frozen couplings?
    - fixed point:
        - cylider equals input?
        - similarity with correct prototype?
        - what are the free neurons doing?
    - learning step:
        - before/after first exposure to an input?
        - exposure to miss-classified input after learning?
    - failure case analysis
        - can readout subnetwork to force onehot be useful?
- Softification:
    - local cross entropy loss to train readout
    - soft update mask based on robustness
- Multilayer:
    - residual connections from readout to hidden layers
    - right fields? ignore right?





- large hidden layers!
    - a) introduce 'extra registers':
        - have an argument N (input size) and an argument H >= N
        - coupling matrices will be HxH, hidden dimension is H
        - ferromagnetic couplings are only active for N units per layer
        - this way, we are able to both propagate information and, potentially, learn representations!
- residual connections with readout?
    - a) singolo readout, teste multiple:
        - introduci una matrice Wback e Wforth per ogni hidden layer
        - il readout può attribuire pesi diversi ai diversi layer (anche 0 per iniziare...)
- implement (optional) soft mask for perceptron rule
- curriculum:
    - ottieni un buon feeling per il rilassamento e l'update
    - prova a concentrare il training sui pattern ancora sbagliati
- dynamics: set up a script to study it!
    - consider initialization, after learning in-distro, after learning out-of-distro
    - convergence? relaxation time? chaos? distance travelled?

(-) ottimizza ancora performance
    - profila su mps!
    - torch.compile?
    - stai copiando memoria? e.g. unfold()
    - stai usando tensori booleani dove puoi?
    - ordine B,L assi?
    - einsum è efficiente?







vecchia lista di idee (altre su notion):
- sparse couplings
- sparse activations (0-1)
- stati continui? -> delta rule
- dinamica rappresentazioni?
- parallelizazione su batch?
- robustezza? a livello del perceptron rule update
- dinamica con message passing?
- coupling binari? confronta con 'regola che trova roba robusta'
