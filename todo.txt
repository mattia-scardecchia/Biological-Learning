TODO:
- raffina nuova implementazione:
    - controlla che faccia quello che pensi
    - attiva learning W e weight decay
    - velocizzala
    - estendila a stati 0/1



Cambiamenti:
- parallelizza! torch?
- impara W --> fallo asimmetrico?
- weight decay

poi:
- sparse couplings
- sparse activations (0-1)
- stati continui? -> delta rule
- dinamica rappresentazioni?
- parallelizazione su batch?
- robustezza? a livello del perceptron rule update
- dinamica con message passing?
- coupling binari? confronta con 'regola che trova roba robusta'


TODO:
- improve logging:
    - track the distribution of couplings (preferrably the whole histogram + asymmetricity, maybe once per epoch)

- matrici J sparse (normalizza bene)
- stati 0-1 (e sparsit√†)