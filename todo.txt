TODO:
- lavora nel notebook:
    - ottieni un buon feeling per il rilassamento e l'update
    - prova a concentrare il training sui pattern ancora sbagliati
- segnale di classificazione su tutti gli strati

(-) ottimizza ancora performance
    - profila su mps!
    - torch.compile?
    - stai copiando memoria? e.g. unfold()
    - stai usando tensori booleani dove puoi?
    - ordine B,L assi?
    - einsum Ã¨ efficiente?

vecchia lista di idee (altre su notion):
- sparse couplings
- sparse activations (0-1)
- stati continui? -> delta rule
- dinamica rappresentazioni?
- parallelizazione su batch?
- robustezza? a livello del perceptron rule update
- dinamica con message passing?
- coupling binari? confronta con 'regola che trova roba robusta'
